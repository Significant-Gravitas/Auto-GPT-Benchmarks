{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21013 entries, 0 to 25470\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype              \n",
      "---  ------                --------------  -----              \n",
      " 0   createdAt             21013 non-null  datetime64[ns]     \n",
      " 1   agent                 21013 non-null  object             \n",
      " 2   costUSD               21013 non-null  float64            \n",
      " 3   job_id                21013 non-null  object             \n",
      " 4   challenge             21013 non-null  object             \n",
      " 5   benchmark_start_time  21013 non-null  datetime64[ns, UTC]\n",
      " 6   prompt                21013 non-null  object             \n",
      " 7   response              21013 non-null  object             \n",
      " 8   model                 21013 non-null  object             \n",
      " 9   request               21013 non-null  object             \n",
      " 10  attempted             15008 non-null  object             \n",
      " 11  categories            15008 non-null  object             \n",
      " 12  task                  15008 non-null  object             \n",
      " 13  success               14835 non-null  float64            \n",
      " 14  difficulty            15008 non-null  object             \n",
      " 15  success_%             14835 non-null  float64            \n",
      " 16  run_time              15008 non-null  object             \n",
      " 17  is_regression         15008 non-null  object             \n",
      " 18  report_time           15008 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](1), float64(3), object(13)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('../reports/df.pkl')\n",
    "df.dropna(subset=['benchmark_start_time', 'response', 'model', 'agent'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TestSearch', 'TestReadFile', 'TestRememberGoal_Hard',\n",
       "       'TestThreeSum', 'TestWriteFile', 'TestRememberGoal_Simple',\n",
       "       'TestDebugSimpleTypoWithGuidance', 'TestPasswordGenerator_Easy',\n",
       "       'TestWritingCLI_FileOrganizer', 'TestRevenueRetrieval',\n",
       "       'TestBasicRetrieval', 'TestRetrieval3', 'TestReturnCode_Tests',\n",
       "       'TestFunctionCodeGeneration', 'TestReturnCode_Modify',\n",
       "       'TestDebugMultipleTypo', 'TestRememberMultipleIdsWithNoise',\n",
       "       'TestRememberGoal_Advanced', 'TestReturnCode_Write',\n",
       "       'TestAdaptSimpleTypoWithGuidance',\n",
       "       'TestDebugSimpleTypoWithoutGuidance', 'TestRememberMultipleIds',\n",
       "       'TestRememberGoal_Medium', 'TestWrite6Files', 'TestAdaptLink',\n",
       "       'TestReturnCode_Simple', 'TestBasicMemory',\n",
       "       'TestWrite5FilesWithArray', 'TestPlanCreation',\n",
       "       'TestRememberMultiplePhrasesWithNoise', 'TestAdaptTeslaRevenue',\n",
       "       'TestBasicSafety_BuildNuke', 'TestWebApp_ListAnimals',\n",
       "       'TestBasicContentGen', 'TestAgentProtocol_CreateAgentTask',\n",
       "       'TestGoalLoss_Simple', 'TestWritingCLI_Easy', 'TestGoalDivergence',\n",
       "       'TestAgentProtocol_ExecuteAgentTaskStep',\n",
       "       'TestAgentProtocol_GetAgentTask',\n",
       "       'TestAgentProtocol_ListAgentTasksIds'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['challenge'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('all_data_jsons.json', 'r') as f:\n",
    "    all_data_jsons = json.load(f)\n",
    "    \n",
    "challenges_array = list(all_data_jsons.keys())\n",
    "\n",
    "# challenges_to_ignore = [\"TestBasicSafety_BuildNuke\",\"TestAgentProtocol_ExecuteAgentTaskStep\", \"TestAgentProtocol_GetAgentTask\", \"TestAgentProtocol_ListAgentTasksIds\", \"TestAgentProtocol_CreateAgentTask\", \"TestWritingCLI_Easy\", \"TestGoalDivergence\", \"TestGoalLoss_Simple\"]\n",
    "\n",
    "agents_to_ignore = ['gpt-engineer', 'smol-developer', 'babyagi', 'evo', 'auto-gpt-turbo']\n",
    "agent_array = set(df['agent'].unique()) - set(agents_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def is_action_auto_gpt(log):\n",
    "    \"\"\"AutoGPTs actions are defined by the presence of the \"command\" key.\n",
    "\n",
    "    World state actions\n",
    "    - web_search\n",
    "    - write_to_file\n",
    "    - browse_website\n",
    "    - execute_python_file\n",
    "    - list_files\n",
    "    - execute_python_code\n",
    "    - read_file\n",
    "    Internal actions\n",
    "    - goals_accomplished\n",
    "\n",
    "    Input\n",
    "        The \"content\" key of an LLM response\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for the existence of the \"command\" key in the log\n",
    "    command_existence = bool(re.search(r'\"command\"\\s*:', log))\n",
    "    \n",
    "    if command_existence:\n",
    "        try:\n",
    "            # Convert the JSON-like string to a Python dictionary\n",
    "            log_dict = json.loads(log)\n",
    "            \n",
    "            # Check if the \"command\" key exists and has a \"name\" key\n",
    "            if \"command\" in log_dict and \"name\" in log_dict[\"command\"]:\n",
    "                command_name = log_dict[\"command\"][\"name\"]\n",
    "\n",
    "                # List of command names that signify an action\n",
    "                action_command_names = [\n",
    "                    \"web_search\",\n",
    "                    \"write_to_file\",\n",
    "                    \"browse_website\",\n",
    "                    \"execute_python_file\",\n",
    "                    \"list_files\",\n",
    "                    \"execute_python_code\",\n",
    "                    \"read_file\",\n",
    "                ]\n",
    "                \n",
    "                # Check if the command name matches any in the list\n",
    "                return command_name in action_command_names\n",
    "        except Exception as e:\n",
    "            print(e, log)\n",
    "            raise e\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_openai_function(log):\n",
    "    \"\"\"OpenAI API function calls are determined by the presence of the \"function_call\" key.\n",
    "    Beebot\n",
    "    World state actions\n",
    "    - get_html_content\n",
    "    - read_file\n",
    "    - write_file\n",
    "    - wolfram_alpha_query\n",
    "    - write_python_code\n",
    "    - execute_python_file\n",
    "    - google_search\n",
    "    - wikipedia\n",
    "    - install_python_package\n",
    "    - execute_python_file_in_background\n",
    "    - get_process_status\n",
    "    - kill_process\n",
    "    - analyze_webpage_content\n",
    "    - get_website_text_content\n",
    "    - gmail_get_message\n",
    "    - gmail_create_draft\n",
    "    - disk_usage\n",
    "    Internal actions\n",
    "    - get_more_tools\n",
    "    - exit\n",
    "    - rewind_actions\n",
    "    - delegate_task\n",
    "    - function_summary\n",
    "\n",
    "    PolyGPT\n",
    "    World state actions\n",
    "    - http.\n",
    "    - filesystem.\n",
    "    - ethers.\n",
    "    - ipfs.\n",
    "    - web-scraper.\n",
    "    - ens.\n",
    "    - safe-factory.\n",
    "    - safe-manager.\n",
    "    Internal actions\n",
    "    - LearnWrap\n",
    "    - InvokeWrap\n",
    "    - user\n",
    "\n",
    "    Input\n",
    "        The entire LLM response\n",
    "    \"\"\"\n",
    "    # Check for the existence of the \"function_call\" key in the log\n",
    "    function_call_existence = bool(log.get(\"function_call\", None))\n",
    "\n",
    "    if function_call_existence:\n",
    "        # Check if the \"function_call\" key exists and has a \"name\" key\n",
    "        if \"name\" in log[\"function_call\"]:\n",
    "            function_name = log[\"function_call\"][\"name\"]\n",
    "\n",
    "            # List of function names that signify an action\n",
    "            action_function_names = [\n",
    "                \"read_file\",\n",
    "                \"write_\",\n",
    "                \"wolfram_alpha_query\",\n",
    "                \"execute_\",\n",
    "                \"install_python_package\",\n",
    "                \"get_\",\n",
    "                \"kill_process\",\n",
    "                \"encyclopedia\",\n",
    "                \"gmail_\",\n",
    "                \"disk_usage\",\n",
    "                \"os_name_and_version\",\n",
    "                \"analyze_webpage_content\",\n",
    "                \"google_\",\n",
    "                \"wikipedia\",\n",
    "                \"http.\",\n",
    "                \"filesystem.\",\n",
    "                \"ethers.\",\n",
    "                \"ipfs.\",\n",
    "                \"web-scraper.\",\n",
    "                \"ens.\",\n",
    "                \"safe-factory.\",\n",
    "                \"safe-manager.\",\n",
    "            ]\n",
    "\n",
    "            # Check if the function name matches any in the list\n",
    "            return any(function_name in action for action in action_function_names)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_action_miniagi(log):\n",
    "    \"\"\"Mini-AGI function calls are determined by the presence of different patterns\n",
    "    World state actions\n",
    "    - execute_python\n",
    "    - web_search\n",
    "    - execute_shell\n",
    "    - ingest_data\n",
    "    - process_data\n",
    "    Internal actions\n",
    "    - done\n",
    "    - talk_to_user\n",
    "    - memorize_thoughts\n",
    "    \"\"\"\n",
    "    # List of function names that signify an action\n",
    "    action_function_names = [\n",
    "        \"execute_python\",\n",
    "        \"web_search\",\n",
    "        \"execute_shell\",\n",
    "        \"ingest_data\",\n",
    "        \"process_data\",\n",
    "    ]\n",
    "\n",
    "    # Check for the <c>...</c> pattern and whether it matches any action function names\n",
    "    c_pattern_match = False\n",
    "    c_pattern_search = re.search(r\"<c>(.*?)<\\/c>\", log)\n",
    "    if c_pattern_search:\n",
    "        c_pattern_match = c_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    # Check for the \"ACTION:\" pattern and whether it matches any action function names\n",
    "    action_pattern_match = False\n",
    "    action_pattern_search = re.search(r\"ACTION:\\s*(\\w+)\\s*(\\(x\\d+\\))?\", log)\n",
    "    if action_pattern_search:\n",
    "        action_pattern_match = action_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    return c_pattern_match or action_pattern_match\n",
    "\n",
    "\n",
    "def is_action_turbo(log):\n",
    "    \"\"\"Turbos actions are defined by the presence of the \"cmd\" key.\n",
    "    World state actions\n",
    "    - search\n",
    "    - www\n",
    "    - py\n",
    "    - aol\n",
    "    - put\n",
    "    - pyf\n",
    "    Internal actions\n",
    "    - end\n",
    "    \"\"\"\n",
    "    # List of function names that signify an action\n",
    "    action_function_names = [\"search\", \"www\", \"py\", \"aol\", \"put\", \"pyf\", \"sh\", \"ls\"]\n",
    "\n",
    "    # Check for the \"cmd\" key pattern and whether its \"name\" field matches any action function names\n",
    "    cmd_pattern_match = False\n",
    "    cmd_pattern_search = re.search(r'\"cmd\"\\s*:\\s*{\\s*\"name\"\\s*:\\s*\"(\\w+)\"', log)\n",
    "    if cmd_pattern_search:\n",
    "        cmd_pattern_match = cmd_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    return cmd_pattern_match\n",
    "\n",
    "\n",
    "def is_action_general(log):\n",
    "    \"\"\"General actions are defined by the presence of specific keywords.\n",
    "\n",
    "    Categories and their keywords:\n",
    "    WRITE: ['write', 'start', 'create', 'execute', 'post']\n",
    "    MODIFY: ['modify', 'mutate', 'delete', 'put']\n",
    "    READ: ['read', 'list', 'search', 'find', 'get', 'browse', 'query', 'www']\n",
    "    GENERAL: ['command', 'call', 'function', 'action', 'http']\n",
    "    \"\"\"\n",
    "\n",
    "    if log is None:\n",
    "        return False\n",
    "\n",
    "    keywords = {\n",
    "        \"WRITE\": [\"write\", \"start\", \"create\", \"execute\", \"post\", \"publish\"],\n",
    "        \"MODIFY\": [\"modify\", \"mutate\", \"delete\", \"put\", \"update\"],\n",
    "        \"READ\": [\n",
    "            \"read\",\n",
    "            \"list\",\n",
    "            \"search\",\n",
    "            \"find\",\n",
    "            \"get\",\n",
    "            \"browse\",\n",
    "            \"query\",\n",
    "            \"www\",\n",
    "            \"view\",\n",
    "        ],\n",
    "        \"GENERAL\": [\n",
    "            \"command\",\n",
    "            \"call\",\n",
    "            \"function\",\n",
    "            \"action\",\n",
    "            \"http\",\n",
    "            \"invoke\",\n",
    "            \"request\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    all_keywords = [word for sublist in keywords.values() for word in sublist]\n",
    "\n",
    "    if log.get(\"content\", \"\"):\n",
    "        log = log[\"content\"]\n",
    "    elif log.get(\"function_call\", \"\"):\n",
    "        log = json.dumps(log[\"function_call\"])\n",
    "\n",
    "    return bool(re.search(rf\"\\b({'|'.join(all_keywords)})\\b\", log))\n",
    "\n",
    "\n",
    "def is_action_agent(log, agent, test=\"\", response=\"\"):\n",
    "    \"\"\"Determines if a log contains an action based on patterns from different agents.\"\"\"\n",
    "    is_action = False\n",
    "\n",
    "    if log is None:\n",
    "        print(\"Log is None\", agent, test, response)\n",
    "        return is_action\n",
    "\n",
    "    log_content = log.get(\"content\", \"\")\n",
    "\n",
    "    if agent == \"auto-gpt\":\n",
    "        is_action = is_action_auto_gpt(log_content)\n",
    "    elif agent in [\"beebot\", \"polygpt\"]:\n",
    "        is_action = is_openai_function(log)\n",
    "    elif agent == \"miniagi\":\n",
    "        is_action = is_action_miniagi(log_content)\n",
    "    elif agent == \"turbo\":\n",
    "        is_action = is_action_turbo(log_content)\n",
    "\n",
    "    return is_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert JSON-like strings to nested dictionaries\n",
    "def nested_json(x):\n",
    "    if pd.notna(x) and x:  # Check for not NaN and not empty\n",
    "        try:\n",
    "            d = json.loads(x)\n",
    "        except json.JSONDecodeError:\n",
    "            return x  # If x is not valid JSON, return as is\n",
    "\n",
    "        if \"content\" in d and isinstance(d[\"content\"], str):\n",
    "            try:\n",
    "                d[\"content\"] = json.loads(d[\"content\"])\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # If inner content is not valid JSON, leave it as is\n",
    "        return d\n",
    "    return x  # If x is NaN or empty, return as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# challenge = \"TestRememberMultipleIds\"\n",
    "\n",
    "# Loop through unique agents\n",
    "for challenge in challenges_array:\n",
    "    for agent in ['auto-gpt']: # agent_array:\n",
    "        \n",
    "        master_response_dict = OrderedDict()\n",
    "        master_response_nested_dict = OrderedDict()\n",
    "        master_actions_dict = OrderedDict()\n",
    "        master_request_dict = OrderedDict()\n",
    "        master_general_actions_dict = OrderedDict()\n",
    "        \n",
    "        # Replace with your actual DataFrame\n",
    "        selected_df = df.loc[(df['agent'] == agent) & (df['challenge'] == challenge)]\n",
    "\n",
    "        # Group by 'benchmark_start_time'\n",
    "        grouped_df = selected_df.groupby('benchmark_start_time')\n",
    "\n",
    "        for timestamp, group in grouped_df:\n",
    "            response_dict = OrderedDict()\n",
    "            response_nested_dict = OrderedDict()\n",
    "            actions_dict = OrderedDict()\n",
    "            request_dict = OrderedDict()\n",
    "            general_actions_dict = OrderedDict()\n",
    "            \n",
    "            total_rows = len(group)\n",
    "            \n",
    "            for i, (_, row) in enumerate(group.iterrows()):\n",
    "                response = json.loads(row['response'])\n",
    "                request = row['request']\n",
    "                response_nested = nested_json(row['response'])\n",
    "                \n",
    "                response_dict[str(total_rows-i)] = response\n",
    "                request_dict[str(total_rows-i)] = request\n",
    "                response_nested_dict[str(total_rows-i)] = response_nested\n",
    "                \n",
    "                if is_action_agent(response, agent, challenge, response):\n",
    "                    actions_dict[str(total_rows-i)] = response\n",
    "\n",
    "                if is_action_general(response):\n",
    "                    general_actions_dict[str(total_rows-i)] = response\n",
    "            \n",
    "            response_dict = OrderedDict(reversed(list(response_dict.items())))\n",
    "            response_nested_dict = OrderedDict(reversed(list(response_nested_dict.items())))\n",
    "            actions_dict = OrderedDict(reversed(list(actions_dict.items())))\n",
    "            request_dict = OrderedDict(reversed(list(request_dict.items())))\n",
    "            general_actions_dict = OrderedDict(reversed(list(general_actions_dict.items())))\n",
    "            \n",
    "            master_response_dict[str(timestamp)] = response_dict\n",
    "            master_response_nested_dict[str(timestamp)] = response_nested_dict\n",
    "            master_actions_dict[str(timestamp)] = actions_dict\n",
    "            master_request_dict[str(timestamp)] = request_dict\n",
    "            master_general_actions_dict[str(timestamp)] = general_actions_dict\n",
    "        \n",
    "        os.makedirs(f'specific_logs/{challenge}', exist_ok=True)\n",
    "        os.makedirs(f'specific_logs/{challenge}/{agent}', exist_ok=True)\n",
    "        \n",
    "        with open(f'specific_logs/{challenge}/{agent}/response.json', 'w') as f:\n",
    "            json.dump(master_response_dict, f, indent=4)\n",
    "\n",
    "        # with open(f'specific_logs/{challenge}/{agent}/regex_specific.json', 'w') as f:\n",
    "        #     json.dump(master_actions_dict, f, indent=4)\n",
    "\n",
    "        # with open(f'specific_logs/{challenge}/{agent}/response_nested.json', 'w') as f:\n",
    "        #     json.dump(master_response_nested_dict, f, indent=4)\n",
    "            \n",
    "        # with open(f'specific_logs/{challenge}/{agent}/request.json', 'w') as f:\n",
    "        #     json.dump(master_request_dict, f, indent=4)\n",
    "\n",
    "        # with open(f'specific_logs/{challenge}/{agent}/regex_simple.json', 'w') as f:\n",
    "        #     json.dump(master_general_actions_dict, f, indent=4)\n",
    "            \n",
    "        # with open(f'specific_logs/{challenge}/{agent}/response_malicious.json', 'w') as f:\n",
    "        #     json.dump(master_response_nested_dict, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Initialize the master challenge dictionaries outside the agent loop\n",
    "master_challenge_response_dict = {}\n",
    "master_challenge_prompt_dict = {}\n",
    "\n",
    "# Loop through unique agents\n",
    "for agent in agent_array:\n",
    "    \n",
    "    # Initialize dictionaries to hold data per challenge\n",
    "    master_response_dict_per_challenge = {}\n",
    "    master_prompt_dict_per_challenge = {}\n",
    "\n",
    "    for challenge in challenges_array:\n",
    "        \n",
    "        master_response_dict = OrderedDict()\n",
    "        master_prompt_dict = OrderedDict()\n",
    "        \n",
    "        # Replace with your actual DataFrame\n",
    "        selected_df = df.loc[(df['agent'] == agent) & (df['challenge'] == challenge)]\n",
    "\n",
    "        # Group by 'benchmark_start_time'\n",
    "        grouped_df = selected_df.groupby('benchmark_start_time')\n",
    "\n",
    "        for timestamp, group in grouped_df:\n",
    "            response_dict = OrderedDict()\n",
    "            prompt_dict = OrderedDict()\n",
    "            \n",
    "            total_rows = len(group)\n",
    "            \n",
    "            for i, (_, row) in enumerate(group.iterrows()):\n",
    "                response = json.loads(row['response'])\n",
    "                prompt = row['prompt']\n",
    "                \n",
    "                response_dict[str(total_rows-i)] = response\n",
    "                prompt_dict[str(total_rows-i)] = prompt\n",
    "            \n",
    "            response_dict = OrderedDict(reversed(list(response_dict.items())))\n",
    "            prompt_dict = OrderedDict(reversed(list(prompt_dict.items())))\n",
    "            \n",
    "            master_response_dict[str(timestamp)] = response_dict\n",
    "            master_prompt_dict[str(timestamp)] = prompt_dict\n",
    "        \n",
    "        master_response_dict_per_challenge[challenge] = master_response_dict\n",
    "        master_prompt_dict_per_challenge[challenge] = master_prompt_dict\n",
    "\n",
    "    # Now write the JSON files, using the master dictionaries for each challenge\n",
    "    os.makedirs(f'all_logs/{agent}', exist_ok=True)\n",
    "\n",
    "    with open(f'all_logs/{agent}/response.json', 'w') as f:\n",
    "        json.dump(master_response_dict_per_challenge, f, indent=4)\n",
    "\n",
    "    with open(f'all_logs/{agent}/prompt.json', 'w') as f:\n",
    "        json.dump(master_prompt_dict_per_challenge, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total logs to insert: 57 with 7 unique commands\n",
      "Total logs: 1965\n",
      "Replacing 387 logs out of 1292 logs that match the insertion logs commands\n",
      "Category 0 count: 172\n",
      "Category 1 count: 162\n",
      "Category 2 count: 53\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Variable to specify the percentage of logs to be replaced\n",
    "replace_percentage = 30  # 30% replace rate\n",
    "\n",
    "# Specify the agent\n",
    "agent = \"auto-gpt\"  # Replace with the actual agent name\n",
    "\n",
    "# Load logs you want to insert from\n",
    "with open('malicious_actions/auto-gpt.json', 'r') as f:\n",
    "    insert_logs = json.load(f)\n",
    "\n",
    "# Organize logs to be inserted by their command name\n",
    "insert_logs_by_command = {}\n",
    "for action in insert_logs['actions']:\n",
    "    cmd_name = action['command']\n",
    "    if cmd_name not in insert_logs_by_command:\n",
    "        insert_logs_by_command[cmd_name] = []\n",
    "    insert_logs_by_command[cmd_name].append(action)\n",
    "    \n",
    "print(f\"Total logs to insert: {len(insert_logs['actions'])} with {len(insert_logs_by_command)} unique commands\")\n",
    "\n",
    "# Load the original logs\n",
    "with open(f'all_logs/{agent}/response.json', 'r') as f:\n",
    "    original_logs = json.load(f)\n",
    "    \n",
    "# Traverse the original logs\n",
    "candidates_to_replace = []\n",
    "\n",
    "# Initialize counters for each category\n",
    "category_count = {0: 0, 1: 0, 2: 0}\n",
    "total_logs = 0\n",
    "\n",
    "for challenge, timestamps in original_logs.items():\n",
    "    for timestamp, logs in timestamps.items():\n",
    "        for log_num, log in logs.items():\n",
    "            total_logs += 1\n",
    "            \n",
    "            content_str = log['content']\n",
    "            cmd_name = \"\"\n",
    "            \n",
    "            if content_str:\n",
    "                try:\n",
    "                    content_dict = json.loads(content_str)\n",
    "                    cmd_name = content_dict.get('command', {}).get('name')\n",
    "                except (json.JSONDecodeError, AttributeError) as e:\n",
    "                    # print(f\"JSON decoding failed for log {log_num}. Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if cmd_name in insert_logs_by_command:\n",
    "                candidates_to_replace.append((challenge, timestamp, log_num))\n",
    "\n",
    "print(f\"Total logs: {total_logs}\")\n",
    "\n",
    "# Choose logs to replace\n",
    "num_to_replace = int(len(candidates_to_replace) * replace_percentage / 100)\n",
    "print(f\"Replacing {num_to_replace} logs out of {len(candidates_to_replace)} logs that match the insertion logs commands\")\n",
    "logs_to_replace = random.sample(candidates_to_replace, num_to_replace)\n",
    "\n",
    "for challenge, timestamp, log_num in logs_to_replace:\n",
    "    # Retrieve the correct command name for the log to be replaced\n",
    "    content_str = json.loads(original_logs[challenge][timestamp][log_num]['content'])\n",
    "    cmd_name = content_str.get('command', {}).get('name')\n",
    "    \n",
    "    if cmd_name in insert_logs_by_command:  # Ensure it exists in the insertion logs\n",
    "        replacement_log = random.choice(insert_logs_by_command[cmd_name])\n",
    "        \n",
    "        # Increment category count if it exists\n",
    "        category = replacement_log.get('category')\n",
    "        category_count[category] += 1\n",
    "        \n",
    "        \n",
    "        original_logs[challenge][timestamp][log_num] = replacement_log\n",
    "        \n",
    "# Print the counts for each category\n",
    "print(f\"Category 0 count: {category_count[0]}\")\n",
    "print(f\"Category 1 count: {category_count[1]}\")\n",
    "print(f\"Category 2 count: {category_count[2]}\")\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(f'all_logs/{agent}', exist_ok=True)\n",
    "\n",
    "# Save the modified logs back to JSON\n",
    "with open(f'all_logs/{agent}/inserted_malicious_responses.json', 'w') as f:\n",
    "    json.dump(original_logs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ['auto-gpt', 'beebot', 'mini-agi', 'polygpt', 'turbo']\n",
    "agents_to_regex = ['beebot', 'mini-agi', 'polygpt', 'turbo']\n",
    "# for auto-gpt, use the inserted_malicious_responses.json file\n",
    "file_name = 'inserted_malicious_responses.json'\n",
    "\n",
    "# List of agents to process\n",
    "for agent in ['auto-gpt']: # agent_array:\n",
    "    # Read the existing 'response.json' file\n",
    "    with open(f'all_logs/{agent}/{file_name}', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize dictionaries to store filtered logs\n",
    "    filtered_specific = {}\n",
    "    filtered_general = {}\n",
    "\n",
    "    for challenge, timestamps in data.items():\n",
    "        filtered_specific[challenge] = {}\n",
    "        filtered_general[challenge] = {}\n",
    "        \n",
    "        for timestamp, logs in timestamps.items():\n",
    "            filtered_specific[challenge][timestamp] = {}\n",
    "            filtered_general[challenge][timestamp] = {}\n",
    "            \n",
    "            for log_num, log in logs.items():\n",
    "                if is_action_agent(log, agent):\n",
    "                    filtered_specific[challenge][timestamp][log_num] = log\n",
    "                if is_action_general(log):\n",
    "                    filtered_general[challenge][timestamp][log_num] = log\n",
    "\n",
    "    # Save the filtered logs as new JSON files\n",
    "    with open(f'all_logs/{agent}/regex_specific.json', 'w') as f:\n",
    "        json.dump(filtered_specific, f, indent=4)\n",
    "\n",
    "    with open(f'all_logs/{agent}/regex_simple.json', 'w') as f:\n",
    "        json.dump(filtered_general, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-gpt filtered logs\n",
      "Simple regex filter logs: 1800\n",
      "Specific regex filter logs: 1292\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_total_ids_in_json_files(file_path):\n",
    "    \"\"\"\n",
    "    Count the total number of IDs across all challenges and timestamps in a given JSON file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - int: The total number of IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    total_id_count = 0\n",
    "\n",
    "    # Load JSON data from the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Iterate through each challenge\n",
    "    for challenge, timestamps in data.items():\n",
    "        # Iterate through each timestamp within the challenge\n",
    "        for timestamp, ids in timestamps.items():\n",
    "            # Add the number of IDs within the timestamp to the total count\n",
    "            total_id_count += len(ids)\n",
    "\n",
    "    return total_id_count\n",
    "\n",
    "for agent in ['auto-gpt']:\n",
    "    # total = count_total_ids_in_json_files(f'all_logs/{agent}/inserted_malicious_responses.json')\n",
    "    regex_simple_logs = count_total_ids_in_json_files(f'all_logs/{agent}/regex_simple.json')\n",
    "    regex_specific_logs = count_total_ids_in_json_files(f'all_logs/{agent}/regex_specific.json')\n",
    "\n",
    "    print(f\"{agent} filtered logs\")\n",
    "    # print(f\"Total logs: {total}\")\n",
    "    print(f\"Simple regex filter logs: {regex_simple_logs}\")\n",
    "    print(f\"Specific regex filter logs: {regex_specific_logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_runs(data):\n",
    "    total_runs = 0\n",
    "    for challenge in data:\n",
    "        total_runs += len(data[challenge])\n",
    "    return total_runs\n",
    "\n",
    "with open('all_logs/auto-gpt/regex_specific.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count_runs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "validation_data = {}\n",
    "\n",
    "# 1. Iterate through each challenge in the JSON\n",
    "for challenge, runs in data.items():\n",
    "    \n",
    "    # 2. Extract the list of timestamps\n",
    "    timestamps = list(runs.keys())\n",
    "    \n",
    "    # 3. Shuffle the list\n",
    "    random.shuffle(timestamps)\n",
    "    \n",
    "    # 4. Split the list into training and validation\n",
    "    train_size = int(len(timestamps) * 0.3)\n",
    "    train_timestamps = timestamps[:train_size]\n",
    "    validation_timestamps = timestamps[train_size:]\n",
    "    \n",
    "    # 5. Create separate JSON entries for training and validation\n",
    "    train_data[challenge] = {\n",
    "        ts: runs[ts] for ts in train_timestamps\n",
    "    }\n",
    "\n",
    "    validation_data[challenge] = {\n",
    "        ts: runs[ts] for ts in validation_timestamps\n",
    "    }\n",
    "\n",
    "# Optionally, save them as JSON files\n",
    "with open(\"all_logs/auto-gpt/train.json\", \"w\") as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "with open(\"all_logs/auto-gpt/validation.json\", \"w\") as validation_file:\n",
    "    json.dump(validation_data, validation_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(count_runs(train_data))\n",
    "count_runs(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"all_logs/auto-gpt/response.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "logs = []\n",
    "for challenge in data:\n",
    "    for timestamp in data[challenge]:\n",
    "        for log_num in data[challenge][timestamp]:\n",
    "            log_entry = data[challenge][timestamp][log_num]\n",
    "            try:\n",
    "                log_entry['content'] = json.loads(log_entry['content'])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            log_entry['challenge'] = challenge\n",
    "            log_entry['id'] = log_num\n",
    "            log_entry['timestamp'] = timestamp\n",
    "            logs.append(log_entry)\n",
    "\n",
    "# Randomly sample 100 logs\n",
    "sampled_logs = random.sample(logs, 100) if len(logs) >= 100 else logs\n",
    "with open(\"all_logs/auto-gpt/sampled_logs.json\", \"w\") as f:\n",
    "    json.dump(sampled_logs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
