{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15386da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    " \n",
    "# a score (0 or 1)\n",
    "# a spectrum (1-10)\n",
    "# a percentage (1-100)\n",
    "\n",
    "# rubric eval\n",
    "# comparison eval\n",
    "\n",
    "def llm_eval(self, evaluation: str) -> float:\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if MOCK_FLAG:\n",
    "            return 1.0\n",
    "\n",
    "    answer = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": evaluation},\n",
    "                ],\n",
    "            )\n",
    "    return answer[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_PROMPT = \"\"\"Ignore previous directions. You are now an expert at evaluating how close machine generated responses are to human answers. You essentially act as a hyper advanced BLEU score. You will assign a score out of 100 to each of the generations internally. Then you will present a score out of 100 overall factoring in all of the scores in the rubric. Use decimal points to be even more accurate.\n",
    "Here is the given task for you to evaluate:\n",
    "{task}\n",
    "\n",
    "Here is the ideal response you're comparing to based on the task:\n",
    "{ideal}\n",
    "\n",
    "Here are some examples of how to score a machine generated response compared to the above ideal response:\n",
    "{examples}\n",
    "\n",
    "Remember to always end your response with nothing but a float score.\n",
    "Float score:\n",
    "\"\"\"\n",
    "\n",
    "RUBRIC_PROMPT = \"\"\"Ignore previous directions. You are now an expert at evaluating machine generated responses to given tasks. You will assign a score out of 100 to each of the generations internally. Then you will present a score out of 100 overall factoring in all of the scores in the rubric. Use decimal points to be even more accurate.\n",
    "Here is the given task for you to evaluate:\n",
    "{task}\n",
    "\n",
    "<Rubric. Use to guide your thinking about scoring.>\n",
    "{rubric}\n",
    "\n",
    "Here are some examples of how to score a machine generated response based on the above rubric.\n",
    "{examples}\n",
    "\n",
    "Remember to always end your response with nothing but a float score.\n",
    "Float score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \n",
    "\n",
    "ideal = \n",
    "rubric = \n",
    "# ground_should_contain\n",
    "# ground should not contain\n",
    "\n",
    "# add {description} and {answer} to the prompt?\n",
    "\n",
    "# options\n",
    "#     reference_eval\n",
    "#     rubric_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_REFERENCE_EXAMPLES = \n",
    "FEW_SHOT_RUBRIC_EXAMPLES = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_evaluation = REFERENCE_PROMPT.format(task=task, ideal=ideal, examples=examples)\n",
    "rubric_evaluation = RUBRIC_PROMPT.format(task=task, rubric=rubric, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faebe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval(reference_evaluation)\n",
    "# llm_eval(rubric_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
